<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title><%= avatar_name %></title>
    <style>
      body {
        margin: 0;
        overflow: hidden;
      }
      canvas {
        display: block;
      }
      #startChatBtn {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        padding: 1rem 2rem;
        font-size: 1.25rem;
        background: #4f46e5;
        color: white;
        border: none;
        border-radius: 0.75rem;
        cursor: pointer;
        box-shadow: 0 6px 18px rgba(0, 0, 0, 0.25);
        z-index: 10;
      }
      #startChatBtn:hover {
        background: #4338ca;
      }
    </style>
  </head>
  <body>
    <button id="startChatBtn">Start Chat</button>

    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.158.0/build/three.module.js",
          "three/examples/jsm/": "https://cdn.jsdelivr.net/npm/three@0.158.0/examples/jsm/"
        }
      }
    </script>

    <!—- same head/style as you already have —->
    <script type="module">
      import * as THREE from "three";
      import { GLTFLoader } from "three/examples/jsm/loaders/GLTFLoader.js";
      import { io } from "https://cdn.socket.io/4.7.2/socket.io.esm.min.js";

      const avatarId = <%= avatar_id %>;
      const glbUrl = "<%= glb_url %>";
      const WS_URL = `${window.location.protocol}//${window.location.host}`;


      console.log("Avatar ID:", avatarId, "GLB:", glbUrl, "WS:", WS_URL);

      /* === THREE + GLB loading (unchanged) === */
      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(15, window.innerWidth/window.innerHeight, 0.1, 1000);
      const renderer = new THREE.WebGLRenderer({ antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);
      scene.add(new THREE.AmbientLight(0xffffff, 0.5));
      const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
      directionalLight.position.set(1, 1, 1);
      scene.add(directionalLight);

      let model, jawBone, headBone, leftArm, rightArm;
      const morphMeshes = [];

      const loader = new GLTFLoader();
      loader.load(glbUrl, (gltf) => {
        model = gltf.scene;
        scene.add(model);
        model.traverse((obj) => {
          if (obj.isBone) {
            const n = obj.name.toLowerCase();
            if (!jawBone && n.includes("jaw")) jawBone = obj;
            if (!headBone && (n === "head" || n.includes("head"))) headBone = obj;
            if (!leftArm && (n.includes("leftarm") || n.includes("upperarm_l") || n.includes("arm_l") || n.includes("shoulder_l"))) {
              leftArm = obj; leftArm.userData.initialRotation = leftArm.quaternion.clone();
            }
            if (!rightArm && (n.includes("rightarm") || n.includes("upperarm_r") || n.includes("arm_r") || n.includes("shoulder_r"))) {
              rightArm = obj; rightArm.userData.initialRotation = rightArm.quaternion.clone();
            }
          }
          if ((obj.isSkinnedMesh || obj.isMesh) && obj.morphTargetDictionary) {
            const targets = [];
            Object.entries(obj.morphTargetDictionary).forEach(([name, idx]) => {
              const key = name.toLowerCase();
              if (key.includes("viseme") || key.includes("mouth") || key.includes("open") || key.includes("phoneme")) {
                targets.push({ name, idx });
              }
            });
            if (targets.length > 0) morphMeshes.push({ mesh: obj, targets });
          }
        });
      }, undefined, (error) => console.error("Error loading GLB:", error));

      camera.position.set(0, 2, 2);
      camera.lookAt(0, 1, -3);
      function animate() { requestAnimationFrame(animate); renderer.render(scene, camera); }
      animate();
      window.addEventListener("resize", () => {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(window.innerWidth, window.innerHeight);
      });

      /* === Audio playback + analyser + queue === */
      let audioCtx = null;
      let analyser = null;
      let isAnimating = false;

      // small WAV header builder (L16 little-endian PCM)
      function makeWavHeader(pcmByteLength, sampleRate = 16000, channels = 1, bitsPerSample = 16) {
        const header = new ArrayBuffer(44);
        const dv = new DataView(header);
        let p = 0;
        function writeString(s) { for (let i=0;i<s.length;i++) dv.setUint8(p++, s.charCodeAt(i)); }
        writeString('RIFF'); dv.setUint32(p, 36 + pcmByteLength, true); p += 4; writeString('WAVE'); writeString('fmt ');
        dv.setUint32(p, 16, true); p += 4; dv.setUint16(p, 1, true); p += 2; dv.setUint16(p, channels, true); p += 2;
        dv.setUint32(p, sampleRate, true); p += 4; dv.setUint32(p, (sampleRate * channels * bitsPerSample) / 8, true); p += 4;
        dv.setUint16(p, (channels * bitsPerSample) / 8, true); p += 2; dv.setUint16(p, bitsPerSample, true); p += 2;
        writeString('data'); dv.setUint32(p, pcmByteLength, true); p += 4;
        return header;
      }

      // decode raw base64 chunk (raw PCM L16) into an AudioBuffer by prepending WAV header
      async function decodeRawPcmBase64ToAudioBuffer(base64) {
        if (!audioCtx) throw new Error("audioCtx not initialized");
        // decode base64 to Uint8Array
        const binary = atob(base64);
        const pcmBytes = new Uint8Array(binary.length);
        for (let i = 0; i < binary.length; i++) pcmBytes[i] = binary.charCodeAt(i);
        // create WAV ArrayBuffer = header + pcmBytes
        const header = makeWavHeader(pcmBytes.length, 16000, 1, 16);
        const wav = new Uint8Array(header.byteLength + pcmBytes.length);
        wav.set(new Uint8Array(header), 0);
        wav.set(pcmBytes, header.byteLength);
        // decode WAV
        return await audioCtx.decodeAudioData(wav.buffer.slice(0)); // slice to ensure transferable
      }

      // simple sequential queue to avoid overlap and to keep analyser connected
      const playQueue = [];
      let playing = false;
      async function enqueueAndPlay(audioBuf) {
        playQueue.push(audioBuf);
        if (playing) return;
        playing = true;
        while (playQueue.length > 0) {
          const buf = playQueue.shift();
          await playBuffer(buf);
        }
        playing = false;
        // signal end: stop analyser-driven animation after small delay
        setTimeout(() => { stopAnimationAndReset(); }, 60);
      }

      async function playBuffer(audioBuffer) {
        if (!audioCtx) return;
        const source = audioCtx.createBufferSource();
        source.buffer = audioBuffer;
        const gain = audioCtx.createGain();
        // make sure analyser exists and is connected
        if (!analyser) {
          analyser = audioCtx.createAnalyser();
          analyser.fftSize = 256;
        }
        // connect: source -> gain -> analyser -> destination
        source.connect(gain);
        gain.connect(analyser);
        analyser.connect(audioCtx.destination);

        return new Promise((resolve) => {
          source.onended = () => { resolve(); };
          source.start();
          // start analyzer-driven mouth animation on first sample
          startAnimationLoop();
        });
      }

      /* === mouth animation driven by analyser === */
      function startAnimationLoop() {
        if (!analyser) return;
        if (isAnimating) return;
        isAnimating = true;
        const data = new Uint8Array(analyser.frequencyBinCount);
        let prev = 0;
        (function step() {
          if (!isAnimating) return;
          analyser.getByteFrequencyData(data);
          // compute a low-mid energy
          let sum = 0;
          for (let i = 1; i < Math.min(10, data.length); i++) sum += data[i];
          let target = THREE.MathUtils.clamp(sum / (9 * 120), 0, 1); // tune denominator to your volume
          // attack/release smoothing
          const attack = 0.25, release = 0.12;
          if (target > prev) prev += (target - prev) * attack;
          else prev += (target - prev) * release;
          const intensity = prev < 0.02 ? 0 : prev;
          if (jawBone) jawBone.rotation.x = -intensity * 0.14; // tuned
          if (morphMeshes.length) {
            morphMeshes.forEach(({ mesh, targets }) => {
              targets.forEach(({ idx }) => {
                if (mesh.morphTargetInfluences && mesh.morphTargetInfluences.length > idx)
                  mesh.morphTargetInfluences[idx] = intensity * 0.6;
              });
            });
          }
          requestAnimationFrame(step);
        })();
      }

      function stopAnimationAndReset() {
        isAnimating = false;
        morphMeshes.forEach(({ mesh, targets }) => {
          targets.forEach(({ idx }) => {
            if (mesh.morphTargetInfluences && mesh.morphTargetInfluences.length > idx)
              mesh.morphTargetInfluences[idx] = 0;
          });
        });
        if (jawBone) jawBone.rotation.x = 0;
      }

      /* === Socket.IO handling === */
      document.getElementById("startChatBtn").addEventListener("click", async () => {
        // create audio context & analyser once on user gesture
        if (!audioCtx) {
          audioCtx = new (window.AudioContext || window.webkitAudioContext)();
          analyser = audioCtx.createAnalyser();
          analyser.fftSize = 256;
          // resume for mobile / browsers that require user gesture
          try { await audioCtx.resume(); } catch (e) { console.warn("audioCtx resume failed:", e); }
        }

        document.getElementById("startChatBtn").style.display = "none";

        const socket = io(WS_URL, { transports: ['websocket'] });
        socket.on("connect", () => {
          console.log("Socket connected:", socket.id);
          socket.emit("join", `avatar.${avatarId}`);
        });

        socket.on("AvatarSpeechChunk", async (chunkBase64) => {
          console.log("AvatarSpeechChunk received (len):", chunkBase64?.length || 0);
          try {
            const buf = await decodeRawPcmBase64ToAudioBuffer(chunkBase64);
            enqueueAndPlay(buf);
          } catch (err) {
            console.error("Failed to decode/play chunk:", err);
          }
        });

        socket.on("AvatarSpeechEnd", () => {
          console.log("AvatarSpeechEnd received");
          // ensure queue plays out and mouth resets
          // (stopAnimationAndReset called after queue drains)
        });

        socket.on("disconnect", () => console.log("Socket disconnected"));
        socket.on("connect_error", (err) => console.error("Socket connect error:", err));
      });
    </script>
  </body>
</html>
